{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mYUPLO_UysSI",
   "metadata": {
    "id": "mYUPLO_UysSI"
   },
   "source": [
    "# Live Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6416c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar modelos\n",
    "detector = hub.load(\"https://tfhub.dev/tensorflow/efficientdet/lite2/detection/1\")\n",
    "labels = pd.read_csv('labels.csv',sep=';',index_col='ID')\n",
    "labels = labels['OBJECT (2017 REL.)']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "#cap = cv2.cvtColor(cap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "width = 448\n",
    "height = 448\n",
    "\n",
    "while(True):\n",
    "    #Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    #Resize to respect the input_shape\n",
    "    inp = cv2.resize(frame, (width , height ))\n",
    "\n",
    "    #Convert img to RGB\n",
    "\n",
    "\n",
    "    rgb = cv2.cvtColor(inp, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #Is optional but i recommend (float convertion and convert img to tensor image)\n",
    "    rgb_tensor = tf.convert_to_tensor(rgb, dtype=tf.uint8)\n",
    "\n",
    "    #Add dims to rgb_tensor\n",
    "    rgb_tensor = tf.expand_dims(rgb_tensor , 0)\n",
    "\n",
    "    boxes, scores, classes, num_detections = detector(rgb_tensor)\n",
    " \n",
    "    pred_labels = classes.numpy().astype('int')[0]\n",
    "    pred_boxes = boxes.numpy()[0].astype('int')\n",
    "    pred_scores = scores.numpy()[0]\n",
    "\n",
    "    # Extract only boxes and labels corresponding to 'person' class\n",
    "    person_boxes = []\n",
    "    person_labels = []\n",
    "    for score, (ymin,xmin,ymax,xmax), label in zip(pred_scores, pred_boxes, pred_labels):\n",
    "        if score < 0.5:\n",
    "            continue\n",
    "        if labels[label] == 'person':\n",
    "            person_boxes.append((ymin, xmin, ymax, xmax))\n",
    "            person_labels.append(labels[label])\n",
    "\n",
    "    # loop throughout the person detections and place a box around it\n",
    "    for box, label in zip(person_boxes, person_labels):\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        img_boxes = cv2.rectangle(inp, (xmin, ymin), (xmax, ymax), (0, 255, 0), 1)      \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img_boxes, label, (xmin, ymax-10), font, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "    #Display the resulting frame\n",
    "    cv2.imshow('black and white', img_boxes)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c52a5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar modelos\n",
    "detector = hub.load(r\"D:\\Python\\models\\Tf_models\")\n",
    "labels = pd.read_csv('labels.csv',sep=';',index_col='ID')\n",
    "labels = labels['OBJECT (2017 REL.)']\n",
    "\n",
    "#labels = ['person']\n",
    "cap = cv2.VideoCapture('D:\\Python\\pexels-pablo-cortés-alegría-3551155-3840x2160-60fps.mp4')\n",
    "#cap = cv2.VideoCapture(0)\n",
    "width = 448\n",
    "height = 448\n",
    "# Define the central region of interest (ROI) by specifying its top-left and bottom-right corners\n",
    "roi_top_left = (44, 44)\n",
    "roi_bottom_right = (402, 402)\n",
    "\n",
    "while(True):\n",
    "    #Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    #Resize to respect the input_shape\n",
    "    inp = cv2.resize(frame, (width , height ))\n",
    "\n",
    "    #Convert img to RGB\n",
    "    rgb = cv2.cvtColor(inp, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #Is optional but i recommend (float convertion and convert img to tensor image)\n",
    "    rgb_tensor = tf.convert_to_tensor(rgb, dtype=tf.uint8)\n",
    "\n",
    "    #Add dims to rgb_tensor\n",
    "    rgb_tensor = tf.expand_dims(rgb_tensor , 0)\n",
    "\n",
    "    boxes, scores, classes, num_detections = detector(rgb_tensor)\n",
    "    #print(classes)\n",
    "    pred_labels = classes.numpy().astype('int')[0]\n",
    "    pred_boxes = boxes.numpy()[0].astype('int')\n",
    "    pred_scores = scores.numpy()[0]\n",
    "    #print(pred_labels)\n",
    "    # Extract only boxes and labels corresponding to 'person' class\n",
    "    person_boxes = []\n",
    "    person_labels = []\n",
    "    for score, (ymin,xmin,ymax,xmax), label in zip(pred_scores, pred_boxes, pred_labels):\n",
    "        if score < 0.5:\n",
    "            continue\n",
    "        if labels[label] == 'person':\n",
    "            person_boxes.append((ymin, xmin, ymax, xmax))\n",
    "            person_labels.append(labels[label])\n",
    "\n",
    "    # loop throughout the person detections and place a box around it\n",
    "    for box, label in zip(person_boxes, person_labels):\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        #print(xmin,ymin)\n",
    "        #print(xmax,ymax)\n",
    "        # Check if the person box is within the central ROI\n",
    "        if (xmin >= roi_top_left[0] and xmin <= roi_bottom_right[0] and \n",
    "            ymin >= roi_top_left[1] and ymin <= roi_bottom_right[1]) or \\\n",
    "            (xmax >= roi_top_left[0] and xmax <= roi_bottom_right[0] and \n",
    "            ymax >= roi_top_left[1] and ymax <= roi_bottom_right[1]) or \\\n",
    "            (xmin >= roi_top_left[0] and xmin <= roi_bottom_right[0] and \n",
    "            ymax >= roi_top_left[1] and ymax <= roi_bottom_right[1]) or \\\n",
    "            (xmax >= roi_top_left[0] and xmax <= roi_bottom_right[0] and \n",
    "            ymin >= roi_top_left[1] and ymin <= roi_bottom_right[1]):           \n",
    "            # Draw the person box in green if it's within the ROI\n",
    "            img_boxes = cv2.rectangle(inp, (xmin, ymin), (xmax, ymax), (0, 255, 0), 1)      \n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(img_boxes, label, (xmin, ymax-10), font, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "            # Draw the person box in red if it's outside the ROI\n",
    "            #img_boxes = cv2.rectangle(inp, (xmin, ymin), (xmax, ymax), (0, 0, 255), 1)      \n",
    "            #font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            #cv2.putText(img_boxes, label, (xmin, ymax-10), font, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Draw the ROI on the frame\n",
    "    roi_frame = cv2.rectangle(inp, roi_top_left, roi_bottom_right, (255, 0, 0), 1)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',roi_frame)\n",
    "\n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f945890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the TFLite model\n",
    "def load_model(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "# Preprocess the input image\n",
    "def preprocess_image(image, size):\n",
    "    image = cv2.resize(image, size)\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.uint8)\n",
    "    return image[tf.newaxis, ...]\n",
    "\n",
    "# Run inference on the preprocessed image\n",
    "def run_inference(interpreter, image):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], image)\n",
    "    interpreter.invoke()\n",
    "    boxes = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "    classes = interpreter.get_tensor(output_details[1][\"index\"])\n",
    "    scores = interpreter.get_tensor(output_details[2][\"index\"])\n",
    "\n",
    "    return boxes, classes, scores\n",
    "def main():\n",
    "    model_path = r\"D:\\Python\\models\\model_lite_2.tflite\"\n",
    "    size = (448,448)  # EfficientDet Lite1 input size is 320x320\n",
    "    width = 448\n",
    "    height = 448\n",
    "    # Load the model\n",
    "    interpreter = load_model(model_path)\n",
    "\n",
    "    # Open webcam\n",
    "    #cap = cv2.VideoCapture('D:\\Python\\pexels-pablo-cortés-alegría-3551155-3840x2160-60fps.mp4')\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    roi_top_left = (44, 44)\n",
    "    roi_bottom_right = (402,402)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        \n",
    "        inp = cv2.resize(frame, (width , height ))\n",
    "        # Preprocess the input image\n",
    "        preprocessed_image = preprocess_image(inp, size)\n",
    "        \n",
    "        # Run inference\n",
    "        boxes, classes, scores = run_inference(interpreter, preprocessed_image)\n",
    "\n",
    "        # Display the results\n",
    "        for i in range(len(boxes[0])):\n",
    "            if scores[0][i] > 0.5 and int(classes[0][i]) == 0:  # Person class has index 0\n",
    "                ymin, xmin, ymax, xmax = boxes[0][i]\n",
    "                \n",
    "                xmin *= preprocessed_image.shape[1]\n",
    "                xmax *= preprocessed_image.shape[1]\n",
    "                ymin *= preprocessed_image.shape[1]\n",
    "                ymax *= preprocessed_image.shape[1]\n",
    "\n",
    "                # Check if the person is within the ROI\n",
    "                if (xmin >= roi_top_left[0] and xmin <= roi_bottom_right[0] and \n",
    "                    ymin >= roi_top_left[1] and ymin <= roi_bottom_right[1]) or \\\n",
    "                    (xmax >= roi_top_left[0] and xmax <= roi_bottom_right[0] and \n",
    "                    ymax >= roi_top_left[1] and ymax <= roi_bottom_right[1]) or \\\n",
    "                    (xmin >= roi_top_left[0] and xmin <= roi_bottom_right[0] and \n",
    "                    ymax >= roi_top_left[1] and ymax <= roi_bottom_right[1]) or \\\n",
    "                    (xmax >= roi_top_left[0] and xmax <= roi_bottom_right[0] and \n",
    "                    ymin >= roi_top_left[1] and ymin <= roi_bottom_right[1]):\n",
    "                    cv2.rectangle(inp, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 2)\n",
    "                    cv2.putText(inp, f\"Person, Score: {scores[0][i]:.2f}\", (int(xmin), int(ymin) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw the ROI on the frame\n",
    "        roi_frame = cv2.rectangle(inp, roi_top_left, roi_bottom_right, (255, 0, 0), 1)\n",
    "        # print(roi_frame.shape)\n",
    "        cv2.imshow(\"Output\",roi_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a377b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "image_detecion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
